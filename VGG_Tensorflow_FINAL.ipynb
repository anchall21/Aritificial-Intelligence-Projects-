{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "jeHHK9ZKNUCn"
      },
      "outputs": [],
      "source": [
        "#Importing library \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "from PIL import Image\n",
        "from imgaug import augmenters as iaa\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "import keras.utils as image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image  \n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow.keras as K\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing library tensorflow to operate on VGG16 model\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "import keras, os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.python.util import deprecation\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "t8Q9Pf_CP9sU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywTPSmkD5Vjc",
        "outputId": "590e5757-2ef2-454b-c0e6-040e03c6fec5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.8/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q6VGb38Ngpj",
        "outputId": "a03e4c8f-214b-4fa0-c129-97c60c012e8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Preparing dictionary for image labels \n",
        "\n",
        "label_map = {0:'Normal', \n",
        "             1:'Atelectasis',\n",
        "             2:'Calcification', \n",
        "             3:'Consolidation', \n",
        "             4:'Effusion', \n",
        "             5:'Emphysema', \n",
        "             6:'Fibrosis', \n",
        "             7:'Fracture', \n",
        "             8:'Mass', \n",
        "             9: 'Nodule', \n",
        "             10:'Pneumothorax'}\n",
        "\n",
        "rev_label_map = {v: k for k, v in label_map.items()}"
      ],
      "metadata": {
        "id": "f_5Dndj9NjJT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Mapping the train dataset labels with index and classes\n",
        "\n",
        "##For mapping, we use json file of labels that were a part of our dataset\n",
        "\n",
        "#In these steps, we first created a dataframe of train labels from json file and then created an array of those labels called y_train\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/train.json') as user_file:\n",
        "    file_contents = user_file.read()\n",
        "parsed_json = json.loads(file_contents)\n",
        "\n",
        "class_list = []\n",
        "file_list = []\n",
        "for i in parsed_json:\n",
        "    class_list.append(list(set(i['syms'])))\n",
        "    file_list.append(i['file_name'])\n",
        "\n",
        "for i in class_list:\n",
        "    if len(i)==0:\n",
        "        i.append(label_map[0])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=class_list)\n",
        "df['label'] = pd.Series(class_list)\n",
        "df = df['label']\n",
        "df_process = pd.get_dummies(df.apply(pd.Series).stack()).sum(level=0)\n",
        "df_process['file_name'] = pd.Series(file_list)\n",
        "df_clean=df_process.drop('file_name',axis = 1)\n",
        "y_train = np.array(df_clean)\n",
        "len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjVv-RnZOKMO",
        "outputId": "03b5a67d-5b99-4316-9ced-f4ca818a27ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-7fc2b8502d92>:26: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
            "  df_process = pd.get_dummies(df.apply(pd.Series).stack()).sum(level=0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3001"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Depiction of how the array of labels look like \n",
        "\n",
        "y_train[:10] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp-GYF7XR0-C",
        "outputId": "a8baf9d1-5608-4efa-8304-a532f3f5f44a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Mapping the test dataset labels with index and classes\n",
        "\n",
        "##For mapping, we use json file of labels that were a part of our dataset\n",
        "\n",
        "#In these steps, we first created a dataframe of test labels from json file and then created an array of those labels called y_test \n",
        "\n",
        "with open('/content/drive/MyDrive/test.json') as user_file:\n",
        "    file_contents = user_file.read()\n",
        "parsed_test_json = json.loads(file_contents)\n",
        "\n",
        "test_class_list = []\n",
        "test_file_list = []\n",
        "for i in parsed_test_json:\n",
        "    test_class_list.append(list(set(i['syms'])))\n",
        "    test_file_list.append(i['file_name'])\n",
        "\n",
        "\n",
        "for i in test_class_list:\n",
        "    if len(i)==0:\n",
        "        i.append(label_map[0])\n",
        "\n",
        "test_df = pd.DataFrame(data=test_class_list)\n",
        "test_df['label'] = pd.Series(test_class_list)\n",
        "test_df = test_df['label']\n",
        "test_df_process = pd.get_dummies(test_df.apply(pd.Series).stack()).sum(level=0)\n",
        "test_df_process['file_name'] = pd.Series(test_file_list)\n",
        "test_df_clean=test_df_process.drop('file_name',axis = 1)\n",
        "y_test = np.array(test_df_clean)\n",
        "len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLmQc5n4NxLv",
        "outputId": "a6b34e58-0281-45cc-b17d-aa14949dc5a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e35d70279e09>:25: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
            "  test_df_process = pd.get_dummies(test_df.apply(pd.Series).stack()).sum(level=0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "542"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Depiction of how the array of labels look like \n",
        "\n",
        "y_test[:10] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eb4rmDuzhj3",
        "outputId": "049b47c9-c293-434c-adce-f8d1faed1915"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We converted the .jpg images of test and train data into a numpy array file for the ease of processing in computer vision models \n",
        "\n",
        "train_data = np.load('/content/drive/MyDrive/xray/train_image.npy', encoding='bytes')\n",
        "test_data = np.load('/content/drive/MyDrive/xray/test_image.npy', encoding='bytes')"
      ],
      "metadata": {
        "id": "aALVwdofQCWd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the shape of training data \n",
        "\n",
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG_7SJdzUKgs",
        "outputId": "d54329f1-aaa2-459e-c4dd-0c6715e333f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3001, 1024, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the shape of testing data \n",
        "\n",
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OKNxORFdOsU",
        "outputId": "c8a73d26-0578-4cff-967f-5ad6c7423186"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(542, 1024, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping the training dataset to reduce their size from 1024 to 224 and from 3-dimensional to 4-dimensional \n",
        "\n",
        "train_new = np.zeros((3001, 300, 300, 1))\n",
        "for i in range(3001):\n",
        "  train_new[i] = np.reshape(cv2.resize(train_data[i], (300,300)), (300,300,1))"
      ],
      "metadata": {
        "id": "I8DxyTIvYNd9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking new shape of training dataset \n",
        "\n",
        "train_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS9y2wtzYrKz",
        "outputId": "9c67a6a8-5d0f-4cae-95a3-624dbd85618c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3001, 300, 300, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping the testing dataset to reduce their size from 1024 to 224 and from 3-dimensional to 4-dimensional \n",
        "\n",
        "test_new = np.zeros((542, 300, 300, 1))\n",
        "for i in range(542):\n",
        "  test_new[i] = np.reshape(cv2.resize(test_data[i], (300,300)), (300,300,1))"
      ],
      "metadata": {
        "id": "VskqLgu7YaLI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking new shape of testing dataset \n",
        "\n",
        "test_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVwNa7p7bg18",
        "outputId": "d4e2ccd8-7f13-444b-fc88-88fe777f2412"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(542, 300, 300, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stratifying data while splitting train and validation data\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "X_train, y_train, X_val, y_val = iterative_train_test_split(train_new, y_train, test_size = 0.1)"
      ],
      "metadata": {
        "id": "MT5saQ8nernp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Setting VGG16 model\n",
        "##Added the last two additional model layers where channels \n",
        "\n",
        "#All the hidden layers use ReLU activation and the last Dense layer uses Sigmoid activation with 11 channels \n",
        "\n",
        "def vgg_16(weights = 'imagenet', include_top = True):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape = (300, 300, 1), filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n",
        "\n",
        "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "    model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units = 4096, activation = \"relu\"))\n",
        "    model.add(Dense(units = 4096, activation = \"relu\"))\n",
        "    model.add(Dense(units = 11, activation = \"sigmoid\"))\n",
        "\n",
        "    opt = Adam(lr = 0.1)\n",
        "   \n",
        "    return model\n",
        "  \n",
        "print(vgg_16().summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_90TYEraULFK",
        "outputId": "dfba95e4-c56f-458f-a77f-1afc9ff930ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 300, 300, 64)      640       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 300, 300, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 150, 150, 64)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 150, 150, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 150, 150, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 75, 75, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 75, 75, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 75, 75, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 75, 75, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 37, 37, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 37, 37, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 37, 37, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 37, 37, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 18, 18, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 9, 9, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 41472)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              169873408 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 11)                45067     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 201,413,323\n",
            "Trainable params: 201,413,323\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up and compiling the model\n",
        "model = vgg_16()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])"
      ],
      "metadata": {
        "id": "loVKxvcVb-QS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing and augmenting the data\n",
        "aug = ImageDataGenerator(horizontal_flip=True, rotation_range=10, featurewise_center=True, featurewise_std_normalization=True,  fill_mode=\"nearest\")\n",
        "aug.fit(X_train)"
      ],
      "metadata": {
        "id": "xJhVQI0LWD_m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training tuned VGG16 model by feeding label dataset through \"aug\" for fitting the model to get training and validation accuracy per epoch\n",
        "\n",
        "batch_size=32\n",
        "model.fit(x=aug.flow(X_train, y_train, batch_size=32),validation_data=(X_val, y_val), steps_per_epoch = len(X_train)//batch_size, epochs = 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKY-6Gqf0DWS",
        "outputId": "60055f89-4066-4c37-f711-d0336739ad07"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "84/84 [==============================] - 4600s 55s/step - loss: 0.3961 - binary_accuracy: 0.8361 - auc: 0.7252 - val_loss: 0.3833 - val_binary_accuracy: 0.8380 - val_auc: 0.7574\n",
            "Epoch 2/10\n",
            "84/84 [==============================] - 4494s 54s/step - loss: 0.3660 - binary_accuracy: 0.8461 - auc: 0.7584 - val_loss: 0.3793 - val_binary_accuracy: 0.8380 - val_auc: 0.7623\n",
            "Epoch 3/10\n",
            "84/84 [==============================] - 4552s 54s/step - loss: 0.3660 - binary_accuracy: 0.8474 - auc: 0.7581 - val_loss: 0.3789 - val_binary_accuracy: 0.8380 - val_auc: 0.7603\n",
            "Epoch 4/10\n",
            "84/84 [==============================] - 4627s 55s/step - loss: 0.3650 - binary_accuracy: 0.8481 - auc: 0.7599 - val_loss: 0.3794 - val_binary_accuracy: 0.8380 - val_auc: 0.7608\n",
            "Epoch 5/10\n",
            "84/84 [==============================] - 4585s 55s/step - loss: 0.3646 - binary_accuracy: 0.8472 - auc: 0.7608 - val_loss: 0.3814 - val_binary_accuracy: 0.8380 - val_auc: 0.7604\n",
            "Epoch 6/10\n",
            "84/84 [==============================] - 4546s 54s/step - loss: 0.3645 - binary_accuracy: 0.8478 - auc: 0.7618 - val_loss: 0.3804 - val_binary_accuracy: 0.8380 - val_auc: 0.7565\n",
            "Epoch 7/10\n",
            "84/84 [==============================] - 4556s 54s/step - loss: 0.3643 - binary_accuracy: 0.8476 - auc: 0.7615 - val_loss: 0.3796 - val_binary_accuracy: 0.8380 - val_auc: 0.7583\n",
            "Epoch 8/10\n",
            "84/84 [==============================] - 4570s 54s/step - loss: 0.3638 - binary_accuracy: 0.8479 - auc: 0.7622 - val_loss: 0.3812 - val_binary_accuracy: 0.8380 - val_auc: 0.7575\n",
            "Epoch 9/10\n",
            "84/84 [==============================] - 4346s 52s/step - loss: 0.3648 - binary_accuracy: 0.8470 - auc: 0.7604 - val_loss: 0.3808 - val_binary_accuracy: 0.8380 - val_auc: 0.7599\n",
            "Epoch 10/10\n",
            "84/84 [==============================] - 4770s 57s/step - loss: 0.3641 - binary_accuracy: 0.8480 - auc: 0.7618 - val_loss: 0.3785 - val_binary_accuracy: 0.8380 - val_auc: 0.7604\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f499e5e28b0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetching probability predictions of each validation and testing images \n",
        "val_pred = model.predict(X_val)\n",
        "np.save('/content/drive/MyDrive/vgg_val_out_final.npy', val_pred)  \n",
        "\n",
        "test_pred = model.predict(test_new)\n",
        "np.save('/content/drive/MyDrive/vgg_test_out_final.npy', test_pred)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rRhy_FS_BOt",
        "outputId": "07e073af-0bf8-4f94-f42a-13af645afaea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 132s 13s/step\n",
            "17/17 [==============================] - 256s 15s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Ending"
      ],
      "metadata": {
        "id": "jU9_5rNUh3DX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}